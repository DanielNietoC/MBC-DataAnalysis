{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxFPNerzM0Bh"
   },
   "source": [
    "### Assignment 1B: Linear regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this assignment, we will see:**\n",
    "- how to assess the impact of an experimental factor on our experimental measurement using **linear regression**\n",
    "- how to take into account the hetereogeneity of our sample population using **linear mixed models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Preparing for this assignment:</b> \n",
    "\n",
    "The following resources will help you to get ready to complete this assignment. \n",
    "<ul>\n",
    "    <li>Watch <a href=\"https://www.youtube.com/watch?v=KsVBBJRb9TE\" target=\"_blank\">this video</a> and then follow with <a href=\"https://www.youtube.com/watch?v=xIDjj6ZyFuw\" target=\"_blank\">this video</a> to learn the concepts about simple linear regression</li>\n",
    "    <li>For a complementary look on linear regression that put its into the more general framework of model fitting, watch <a href=\"https://www.youtube.com/watch?v=9JfXKmVB6qc\" target=\"_blank\">this video</a> (until 15:10) and then follow with <a href=\"https://www.youtube.com/watch?v=HumajfjJ37E\" target=\"_blank\">this video</a> (linear regression by minimizing squared errors) and <a href=\"https://www.youtube.com/watch?v=8mpNmzLKNfU\" target=\"_blank\">this one</a> (linear regression by maximizing likelihood). These are parts of the material for Model Fitting day which are accessible <a href=\"https://compneuro.neuromatch.io/tutorials/W1D2_ModelFitting/chapter_title.html\" target=\"_blank\">here</a>. </li>\n",
    "    <li> (Very optional) See <a href=\"https://www.cns.nyu.edu/~eero/NOTES/leastSquares.pdf\" target=\"_blank\">these notes</a> from Eero Simoncelli (NYU) if you want to delve more into the mathematics related to linear regression</li>\n",
    "    <li>Read <a href=\"https://doi.org/10.1152/jn.00362.2015\" target=\"_blank\">this paper</a> (focussing on Experiment 2) to understand the logic of the experimental data that we will analyze</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load again the working memory dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and organize it in a Pandas dataframe (as in Assignment 1A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def circdist_rad(angles1,angles2): #define the name and set the arguments between parentheses\n",
    "    output = np.angle( np.exp(1j*(angles1-angles2))) ## mathematical operation to get the circular distance\n",
    "    return output #return the circular distance in radians \n",
    "\n",
    "# load the data from github\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A0_PythonBasics/Experiment2_all_subjects.csv\",sep=' ')\n",
    "\n",
    "# add errors and CW/CCW info\n",
    "df['error'] = circdist_rad(df['response'],df['target'])\n",
    "df['dist_closest'] = circdist_rad(df['distr1'],df['target'])\n",
    "df['cw'] = df['dist_closest']<0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2vy8UPOM0EV"
   },
   "source": [
    "The analysis so far (in Assignment 1A) was simple and good. However, one thing it did not include was all the detail about the location of non-targets. The design of our experiment was not really binary, with just one degree of freedom for CW or CCW non-targets. Instead, they were located at variable distances both in CW and CCW trials. See this by plotting the histogram of 'dist_closest', the distances between non-target and target items in each trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:53.865253Z",
     "start_time": "2022-02-04T22:46:53.712996Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gD3Xhl_NM0EW",
    "outputId": "5943e860-7570-407f-a5ce-fd4d5eadd0b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gn8u9WUMM0Ec"
   },
   "source": [
    "This variability in 'dist_closest' (X) could be a factor explaining the variability in response error Y.\n",
    "The appropriate analysis to check this dependency is a “simple linear regression”. A regression tests for the contribution of the variability in an independent variable (here the inter-item distance X, which is independently set by the experimenter) to generating the variability in dependent variable (here the behavioral error Y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will ignore that data points come from different subjects. Run the analysis for all the data together and obtain the coefficients of the linear regression that fits the data.  We can run this analysis with the function *ols* in **statsmodel.formula.api**. Google it to see how it is used.\n",
    "\n",
    "By using the dataframe you just have to define the function of the regression that you want to use. Which one is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:53.929474Z",
     "start_time": "2022-02-04T22:46:53.866909Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dONKI852M0Ed",
    "outputId": "d9e824c3-8060-443d-f4ad-5295ab98e8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: Intercept      -0.027865\n",
      "dist_closest    0.069416\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "mod = ols(formula = ??? , data=df)\n",
    "res = mod.fit()\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', res.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COb3zPghM0Ei"
   },
   "source": [
    "Can we say that the regression is significant? (i.e. is the slope of the regression line significantly different from zero?). This is an inference question that we will see if we can resolve, we will first need to test if the assumptions of the inference are met in our data. But before that, we can get a first sense of our regression result graphically. Always, graphical representations help in identifying trends in your data or pointing towards possible problems.\n",
    "\n",
    "Use the regression coefficients to plot the regression line on top of the actual data points (X,Y). Is the result in agreement with the analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.193562Z",
     "start_time": "2022-02-04T22:46:53.931341Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "6qxrAS2zM0Em",
    "outputId": "ad6287ab-249e-4eeb-87ca-8c8156d6a3f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x_data = ??? # set x-values for your prediction, these can be the same X values used for estimation or others for your desired range of prediction\n",
    "y_data = ??? + ??? * x_data # given the values in x_data, predict y_data values from our estimated linear model parameters in *res_params*\n",
    "plt.plot(x_data, y_data, 'r') # plot the model prediction with a line\n",
    "plt.plot( ???, ???, 'bo') # now plot the true data from our dataframe df as dots\n",
    "plt.ylim(-0.5, 0.5); # plot limits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-i_ThucM0Er"
   },
   "source": [
    "Should we believe this slope? Is our data really supporting a slope different than zero? \n",
    "Before we actually do inference on the parameter estimates of the model, we need to worry about the assumptions of the test. Regression is a parametric test and assumes a **normal distribution of independent residual errors**. We usually perform two checks: we test independence of errors by plotting residuals against the independent variable, and we check the Gaussian distribution of residuals with *probplot*. Watch the [second video on linear regression above](https://www.youtube.com/watch?v=xIDjj6ZyFuw) to understand these checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.395317Z",
     "start_time": "2022-02-04T22:46:54.195342Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NLT4S6HHM0Es",
    "outputId": "bb22e6eb-7bea-4762-c168-256da9064b39"
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import probplot\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(???, res.resid_pearson) # we first plot the residuals versus the independent variable X in a scatter plot\n",
    "plt.ylabel('model residuals')\n",
    "plt.xlabel('model''s independent variable (X)')\n",
    "plt.subplot(1,2,2)\n",
    "probplot(res.resid_pearson, plot=plt); # then we plot the QQ-plot to check Gaussian statistics\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCe6F2rgM0Ey"
   },
   "source": [
    "Errors do not appear to have any strong dependency on X (left graph), but the graph on the right shows that the distribution of residuals is slightly non-Gaussian. In this case this is a very minor deviation and we would generally validate the assumptions of our inference tests, here. But let's for now worry about whether this could violate the assumptions of the linear regression method. Could it be about outliers?\n",
    "\n",
    "\n",
    "We will thus control the outliers to make sure that they are not affecting the result. We thus repeat the analysis without outliers. To identify outliers we will use the *zscore* function. Google about it to learn its meaning. A typical threshold for outliers is 3 standard deviations from the mean, in either direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.633095Z",
     "start_time": "2022-02-04T22:46:54.396812Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "s72XFArDM0E2",
    "outputId": "f03f1ad8-033c-4303-a979-ba71f0ad9930",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import zscore # we import the zscore function\n",
    "\n",
    "# get the number of upper outliers \n",
    "upp_out = sum(zscore(df.error) > 3 )\n",
    "print('Number of Upper outliers : ' +str(upp_out))\n",
    "\n",
    "# get the number of lower outliers\n",
    "low_out = sum(zscore(df.error) < -3)\n",
    "print('Number of Lower outliers : ' +str(low_out))\n",
    "\n",
    "# now get the dataset excluding outliers\n",
    "rem_out = abs(zscore(df.error)) < 3. # boolean for the non-outlier data rows\n",
    "df_out = df[rem_out].reset_index(drop=True) # data without outliers\n",
    "\n",
    "# and we run the regression analysis\n",
    "mod_out = ols(formula='error ~ dist_closest', data=df_out)\n",
    "res_out = mod_out.fit()\n",
    "\n",
    "#and we test if the new data conforms to the assumptions of the inference test\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(df_out.dist_closest, res_out.resid_pearson,'.')\n",
    "plt.ylabel('model residuals')\n",
    "plt.xlabel('model''s independent variable (X)')\n",
    "plt.subplot(1,2,2)\n",
    "probplot(res_out.resid_pearson, plot=plt); # then we plot the QQ-plot to check Gaussian statistics\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvPQUUeWM0E6"
   },
   "source": [
    "After removing 7 outliers in our data, we now find that the residuals of our model are distributed following a Gaussian distribution approximately, as the blue dots fall on the red line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three examples in the figure below illustrate 3 different situations with outliers. In case 1, the outlier is totally determining the regression so that if we drop it, the effect is gone (we cannot reject the horizontal line as best fit). In this case the outlier HAS to be dropped and you are not allowed to report any effect. This are the bad outliers. In case 2, the outlier has an effect on your data (the slope of the regression changes with and without the outlier, and the quality of the fit, too), but it does not change qualitatively your conclusions (still positive regression slope). You are allowed to remove the outlier but you should report the two values of the slope, when you have it and when you don't. The best case is case 3, where independently of whether you include the outlier or not, the result of your analysis is the same. In this case you can drop the outlier safely. In any case, you have to report that there was an outlier and that you dropped it for such and such reasons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Outlier](https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A1_LinearRegression/outlier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once we are satisfied that the modeling of our data conforms to the assumptions of our inference tests, we are safe to inspect and interpret the results of the inference on the estimated parameters of our linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.647419Z",
     "start_time": "2022-02-04T22:46:54.637164Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# we show the results\n",
    "print( res_out.summary() )\n",
    "print()\n",
    "print(\"p-value for the slope: p=\" + str(res_out.pvalues[1]))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you interpret from this table? Open a *Markdown cell* here below and specify your interpretations. If you look again at [the video indicated above](https://www.youtube.com/watch?v=xIDjj6ZyFuw), you will get ideas as to how these tables are interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Linear regression with categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klu1tOD6M0E8"
   },
   "source": [
    "Now we realize that in the regression analysis we included data points that were not truly independent, as they were collected in 9 different subjects. We thus want to include that part of the design in the regression analysis, as it could increase the statistical power. When we do that, we will need to use a **general linear model**, a linear model that includes factors of mixed nature, some being categorical and some continuous (as in regression). This is the most common situation and a general analysis tool that encompasses all the previous techniques in this assignment. To include subject identity in the design of a general linear model, we need to define the regressors as categorical, because otherwise it would be regressing orderly along our subject numbers. This is really simple by using the formula. Writting C('factor') instead of 'factor' makes it categorical. Implement that in the regression, test the assumptions of the inference tests and then get the inference table for the model estimates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.937437Z",
     "start_time": "2022-02-04T22:46:54.651066Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "v2vZeNuvM0FA",
    "outputId": "c3464ba7-aa17-4505-9073-47855a1e1ab1"
   },
   "outputs": [],
   "source": [
    "# we define and estimate the model\n",
    "mod_sub = ols(formula= ??? , data=df_out)\n",
    "res_sub = mod_sub.fit()\n",
    "\n",
    "# we assess graphically if the model residuals conform to inference assumptions (just as we did before!)\n",
    "???\n",
    "???\n",
    "???\n",
    "???\n",
    "???\n",
    "???\n",
    "???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.943241Z",
     "start_time": "2022-02-04T22:46:54.939949Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.round(res_sub.params[6],5)==-0.01813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:54.969987Z",
     "start_time": "2022-02-04T22:46:54.945081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  error   R-squared:                       0.039\n",
      "Model:                            OLS   Adj. R-squared:                  0.029\n",
      "Method:                 Least Squares   F-statistic:                     3.773\n",
      "Date:                Wed, 18 Jan 2023   Prob (F-statistic):           0.000113\n",
      "Time:                        18:20:28   Log-Likelihood:                 466.54\n",
      "No. Observations:                 850   AIC:                            -913.1\n",
      "Df Residuals:                     840   BIC:                            -865.6\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            -0.0019      0.016     -0.117      0.907      -0.034       0.030\n",
      "C(subject)[T.1.0]     0.0124      0.026      0.469      0.639      -0.039       0.064\n",
      "C(subject)[T.2.0]    -0.0584      0.022     -2.687      0.007      -0.101      -0.016\n",
      "C(subject)[T.3.0]    -0.0113      0.023     -0.484      0.629      -0.057       0.035\n",
      "C(subject)[T.4.0]    -0.0072      0.023     -0.307      0.759      -0.053       0.039\n",
      "C(subject)[T.5.0]    -0.0449      0.022     -2.001      0.046      -0.089      -0.001\n",
      "C(subject)[T.6.0]    -0.0181      0.019     -0.951      0.342      -0.056       0.019\n",
      "C(subject)[T.7.0]    -0.0361      0.021     -1.714      0.087      -0.077       0.005\n",
      "C(subject)[T.8.0]    -0.0202      0.021     -0.980      0.327      -0.061       0.020\n",
      "dist_closest          0.0742      0.017      4.377      0.000       0.041       0.107\n",
      "==============================================================================\n",
      "Omnibus:                        4.417   Durbin-Watson:                   1.374\n",
      "Prob(Omnibus):                  0.110   Jarque-Bera (JB):                5.357\n",
      "Skew:                           0.001   Prob(JB):                       0.0687\n",
      "Kurtosis:                       3.389   Cond. No.                         11.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "p-value for the slope: p=1.3570890925375023e-05\n"
     ]
    }
   ],
   "source": [
    "# we print the results of the inference\n",
    "print( res_sub.summary() )\n",
    "print(\"p-value for the slope: p=\" + str(res_sub.pvalues[???]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can you find the p-value for the regressor of interest? Why are the other regressors not “of interest”? Why do we include them, then? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Syq7l0DAM0FE"
   },
   "source": [
    "\n",
    "In this assignment we have dealt with adding regressors that are not of interest *per se* in our analysis, but that we include in order to improve the model fit for our factor of interest. This is the classic condition for the “subject” part of the design (which, in addition could be treated as a random factor, try and read about that when you get to use these methods in your own data). In Assignment 2, we will work with designs that have several factors of interest, and we will analyze the concept of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extension exercises (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear mixed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used categorial variables to model differences between subjects in our experiment. As the number of subjects increases, this becomes a not very optimal approach to modeling subject-to-subject variability in our data because we will include more and more parameters in the model (one per subject, essentially). Since we are typically not interested in the specific parameter of one or another subject but on a general modeling of the population variability, what mathematicians have figured is that these factors in the design should be modeled as \"random factors\" so that instead of estimating one parameter per subject, one would estimate the variance of the population (assuming, again, Gaussian statistics). In this way, one parameter of variance will take care of the variability across participants. Linear models that include both fixed factors (one parameter per regressor) and random factors (a variance parameter accounting for a family of regressors) are called \"mixed models\". These are usually the way that we should model subjects in our designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore how to do that in this extension exercise. This is a bit tricky as these methods are advanced and are not very well developed in Python. The R language is a lot better adapted for advanced statistics methods. You have two choices:\n",
    "1) Use the function *mixedlm* in the Python library **statsmodels.formula.api**. This should be very parallel to what you just did, but you need to choose the right options for *mixedlm*\n",
    "\n",
    "2) Use the Python library **pymer4.models**, which gives you access to R functions from within Python. You can then import the function *Lmer* and call it specifying the fixed and random factors in your design (check the documentation). This is a more powerful approach, because of the simplicity of the syntax and all the power of R that you can invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:55.260351Z",
     "start_time": "2022-02-04T22:46:54.971773Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OawmGQU2M0FF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Testing the linearity of the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you think carefully, the linear regression analysis above was not really fair. We cannot really conclude that small differences in distance between target and distractor affect the error bias beyond the CW/CCW difference. This is because the data has such a strong cluster structure and our linear analysis may just be picking the two clusters and not finer structrure within clusters. In order to really see if the small variance of target-distraction distances had an impact in error bias, we should look at regression within each of the two CW/CCW categories. One way to address that is to fold the errors: if we take negative target-distraction distances and \"fold\" them onto the positive axis, while the corresponding y values (error) are also changed in sign, we will collapse the two clouds onto one single compact cloud while keeping the possible linear relationship. Then we can run a linear regression analysis on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this analysis with any of the linear regression models that we have tried above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:47:00.621204Z",
     "start_time": "2022-02-04T22:47:00.608879Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment_4-full.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
