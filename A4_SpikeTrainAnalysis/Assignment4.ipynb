{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "## Spike train analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Preparing for this assignment:</b> \n",
    "\n",
    "These resources will help you prepare for this assignment. Watch:\n",
    "<ul>\n",
    "        <li> <a href=\"https://youtu.be/smHwRzk81b0?t=688\" target=\"_blank\">this video</a> (until 1:01'54) from a MIT course by Prof. Michale Fee to learn about key concepts for spike train analysis: the <b>Poisson process</b>, <b>Fano Factor</b>, <b>inter-spike intervals</b>, <b>cross-correlation</b> and <b>auto-correlation</b>. You do NOT need to follow the equations, just the concepts. </li> \n",
    "        <li> <a href=\"https://youtu.be/m1w7oywzwpA\" target=\"_blank\">the first part of this video</a> (until 17'15) to learn about Generalized Linear Models (GLM) and in particular about the <b>Poisson GLM for spike count regression</b>. Things get a bit more mathy after 10'12: again, forget about the equations if you cannot follow them and try to focus on the concepts. Remember that we have used GLMs previously: linear regression and logistic regressions are classes of GLMs, for continuous and binary data respectively.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "In this assignment we will learn about tools to visualize, analyze and model spiking data. Spike trains are different from typical type series such as EEG or fMRI data as they consist of the list of timings for discrete events (action potentials). The same tools can be applied to analyze other event data such as eye saccades, or lever presses in animal conditioning experiments.\n",
    "\n",
    "The data will be used are spiking activity collected from **one neuron of the dorso-medial striatum** (a structure of the basal ganglia) of one rat while it performs a **perceptual decision-making task**. In one of the analysis we will also look at another neuron recorded during the same experimental session. The (complex) details of the behavioral protocol are available in this [publication](https://www.nature.com/articles/s41467-020-14824-w), but for all we care here **the animal basically has to go to a left or right port depending on whether the dominant tone in an acoustic stimuli is high frequency or low frequency**. The difficulty of the trial is manipulated in a variable called \"stimulus evidence\" (or simply Stimulus in the csv file), bounded between -1 and 1. Its value is +1 for pure high-frequency stimulus (i.e. clear evidence towards the rightward response), -1 for pure low-frequency stimulus  (i.e. clear evidence towards the leftward response), and values in between for a mix of low-frequency and high-frequency stimulus (for example stimulus 0 means no evidence at all for either response).\n",
    "\n",
    "Our goal here is to study the basic characteristics of this neuron spiking activity and **whether the neuron activity encodes for the stimulus information and/or the response of the animal**.\n",
    "\n",
    "Let us first import the typical packages (plus some others we will need along the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T16:11:07.623421Z",
     "start_time": "2021-03-09T16:11:07.194795Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import expon, norm # for exponential and gaussian distributions\n",
    "from scipy.signal import correlate, correlation_lags # for auto-correlation / cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the spiking data and the behavioral data. The spiking data will be loaded as a simple numpy array which represents the timing (in seconds) of all spikes for this neuron during this session. The behavioral data is loaded as a dataframe, comprising the following variables: `Stimulus` (stimulus evidence), `Response` (1: left; 2:right), `Outcome` (0: incorrect, 1:correct), `tResponse` (the timing of the onset of response, in seconds). This last variable will help us isolate the neuron spikes that occur around each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T16:11:08.209034Z",
     "start_time": "2021-03-09T16:11:08.179587Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load spiking data as numpy array\n",
    "SpikeTimes =  np.loadtxt(\"https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A4_SpikeTrainAnalysis/Neuron1_Spike.csv\")\n",
    "\n",
    "# load behavioral data as dataframe\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A4_SpikeTrainAnalysis/BehavioralData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic measures\n",
    "## 1.1 Displaying spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many spikes did we record in total for this neuron?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the spike times in a certain window**, to get a rough idea of how this neuron fires.\n",
    "Adjust the window to look at the activity at different time scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define window (start time - end time)\n",
    "win = ??\n",
    "\n",
    "# Select all spikes within this window\n",
    "SpikesInWindow = ??\n",
    "\n",
    "# plot as vertical ticks\n",
    "plt.plot(???);\n",
    "\n",
    "# add labels\n",
    "plt.xlabel('time (s)');\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Computing spike rate\n",
    "**Compute the average spike rate of this neuron**, i.e. the total number of spikes divided by the length of the time window (in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of window\n",
    "WinLength = ??\n",
    "\n",
    "# spike rate\n",
    "SpikeRate = ??\n",
    "\n",
    "print(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Distribution of inter-spike intervals\n",
    "**Plot the distribution of inter-spike intervals** (i.e. the time intervals between two successive spikes). Hints: use the function `np.diff`; to plot a distribution, use the function `plt.hist` and set the parameter `density` to `True`.\n",
    "\n",
    "We know that if the neuron spikes according to an homogeneous Poisson process, this distribution should be exponential (with a scale equal to the inverse of the neuron spike rate).\n",
    "**Plot the prediction for the ISI distribution for the Poisson process on top of the histogram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the array inter-spike intervals\n",
    "ISI = ??\n",
    "\n",
    "# plot the experimental distribution of ISI (adjust the number of bins)\n",
    "plt.hist(???, label = 'experimental')\n",
    "\n",
    "# scale of the Poisson process\n",
    "scale =??\n",
    "\n",
    "# array of ISI values for which we want to compute the predicted pdf\n",
    "ISIrange = np.arange(??)\n",
    "\n",
    "# corresponding values for probability distribution\n",
    "p_Poisson = expon.pdf(ISIrange, scale=scale)\n",
    "\n",
    "# plot as red curve\n",
    "plt.plot(??, label='Poisson'); \n",
    "\n",
    "# add label\n",
    "plt.xlabel(??);\n",
    "plt.ylabel(??);\n",
    "plt.legend;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auto-correlation and cross-correlation\n",
    "\n",
    "## 2.1 Auto-correlation\n",
    "\n",
    "We want to understand better the dynamics of spiking activity, using the tool of auto-correlation. As explained in the first video, auto-correlation tells us about the **time scales that regulate spiking activity**: whether there are periods of higher firing (due to either external factors, i.e. the presence of a stimulus to which the neuron responds; or internal factors, i.e. increased excitability in the local neural network), and the time scales of this period. Remember: the auto-correlation of the (really dull) homogeneous Poisson process is completely flat at 0 (except for the value at zero-lag, as a variable is always correlated with itself).\n",
    "\n",
    "Auto-correlation (or cross-correlation) tools do not work directly on spiking data but on discrete time series (like EEG,...). So our first step is to \"bin spikes\", i.e. define regular time bins of a certain duration and compute the number of spikes in each bin. **Bin the neuron spikes with a time resolution of 10 ms** (i.e. time bin duration = 10 ms). Hint: use `np.histogram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time bin duration (in SECONDS!!)\n",
    "dt = ??\n",
    "\n",
    "# define our time grid, i.e. an array of values spanning all the recording time interspersed with dt\n",
    "time_grid = np.arange(??)\n",
    "\n",
    "# compute the number of spikes in each bin\n",
    "SpikeCount = ???\n",
    "\n",
    "# display first 50 values\n",
    "SpikeCount[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure it worked\n",
    "assert(SpikeCount[24]==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **compute the auto-correlation of the signal** using the function `correlate` from the `scipy` package.\n",
    "We need to compute the measure on **centered data** (i.e. removing the mean of the value to the spike count data) so that the null values correspond to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center the spike count data, i.e. remove its mean value\n",
    "SpikeCountCentered = ??\n",
    "\n",
    "# compute the auto-correlation (i.e. cross-correlation between the two same signals)\n",
    "AC = correlate(???)\n",
    "\n",
    "# this provides the values of the corresponding lags\n",
    "lags = correlation_lags(len(SpikeCountCentered), len(SpikeCountCentered))\n",
    "\n",
    "# lags are given in time steps, convert to seconds\n",
    "lags = ??\n",
    "\n",
    "# plot\n",
    "plt.plot(???);\n",
    "plt.xlabel(??);\n",
    "plt.ylabel(??);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice to have made it, but we can't really see anything because the auto-correlation covers the whole range of timing in the data (up to 4000 seconds, i.e. over an hour of recording). In reality we don't expect the most significant dynamics to last more than one second, so let's **plot the auto-correlation only for lags up to one second**.\n",
    "\n",
    "Moreover, the peak corresponds to the value at exactly lag-zero, which is very not informative because at lag-zero one variable is always maximally correlated to each other. So we will **change this value to a nan** (use `np.nan`) to avoid distorting the scale along the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define maximal lag\n",
    "MaxLag = ??\n",
    "\n",
    "# set value at lag 0 to nan (we need to convert first to floater)\n",
    "AC = np.float64(AC)\n",
    "???\n",
    "\n",
    "# boolean array for all lags smaller than maximal lag (in absolute value)\n",
    "mask = ???\n",
    "\n",
    "# plot the AC again\n",
    "plt.plot(???);\n",
    "plt.xlabel(??);\n",
    "plt.ylabel(??);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret the plot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cross-correlation\n",
    "\n",
    "Auto-correlation is like cross-correlation applied to the same signal, but what is cross-correlation? Cross-correlation allows to look for **interactions between two signals not only at corresponding times, but also with certain time lags**. For example, if one neuron receives excitatory synaptic input from another neuron, on general that higher activity in the pre-synaptic neuron will be followed with higher activity in the post-synaptic neuron (with a certain lag set by synaptic dynamics). This will be directly visible in the cross-correlogram as a bump occurring at the corresponding lag.\n",
    "**Warning!** while it is tempting to interpret a cross-correlogram as a causal interaction between the two signals, cross-correlograms can also signal shared influence impacting both signals. In fact in neural activity, the probability that two neurons are connected by a direct synapse is rather low, so more often than not a cross-correlogram reveals the influence from shared incoming neural signals.\n",
    "\n",
    "Load the spiking activity from neuron 4 and **compute the cross-correlogram between the two neurons**.\n",
    "We will first **define a function to compute and plot the cross-correlogram**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a function to plot the cross-correlation between two signals\n",
    "def plot_crosscorrelation(X, Y, dt, MaxLag):\n",
    "    \"\"\"\n",
    "    Plots the cross-correlation between two signals \n",
    "    Args:\n",
    "       X (array): first signal (time series)\n",
    "       Y (array): second signal (time series)\n",
    "       dt (float): time resolution (in sec)\n",
    "       MaxLag (float): maximum lag to be plotted (in sec)\n",
    "    \"\"\"\n",
    "    # center the two signals X and Y\n",
    "    X =?? \n",
    "    Y = ??\n",
    "    \n",
    "    # compute the cross-correlation and lags\n",
    "    CC = correlate(??)\n",
    "    lags = correlation_lags(len(X), len(Y))\n",
    "    \n",
    "    # convert lags to seconds\n",
    "    ???\n",
    "    \n",
    "    # define mask for lags below value set by MaxLag\n",
    "    ???\n",
    "\n",
    "    # plot\n",
    "    ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data from neuron 4\n",
    "SpikeTimes4 =  np.loadtxt(\"Neuron4_Spike.csv\")\n",
    "\n",
    "# Bin spikes (using same time grid as for neuron 1)\n",
    "SpikeCount4 = ????\n",
    "\n",
    "# use function created above to plot the cross-correlation\n",
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 (Optional) Cross-correlation on synthetic data\n",
    "To understand better how cross-correlation, let's use it on simulated data. \n",
    "First let us simulate one neuron that spikes as a Poisson process with the same rate as neuron 1 in our experimental data.\n",
    "\n",
    "**Simulate the spike counts in each bin for this synthetic neuron.** A Poisson process can be simulated using `np.random.poisson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average spike count across time bins\n",
    "AverageCount = ??\n",
    "\n",
    "# generate an array of spike count following Poisson process with corresponding rate\n",
    "SyntheticSpike = np.random.poisson(??, ??)\n",
    "\n",
    "# display first 100 bins\n",
    "SyntheticSpike[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now simulate the activity of a second (extremely over-simplified!) neuron receiving input from the first neuron and which outputs the exact same spike train as the input with a certain delay of 30 ms (so 3 time bins).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lag (in time bins)\n",
    "lag = ??\n",
    "\n",
    "# Define activity of second neuron (Hint: we just need to add zeros at the beginning of the spike count array, and remove the corresponding values at the end)\n",
    "SyntheticSpike2 = ???\n",
    "\n",
    "# display first 100 bins (check that indeed spikes in neuron 1 are lagged in neuron 3 by 3 bins)\n",
    "SyntheticSpike2[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now plot the cross-correlation between the spike counts of the two neurons** (using our previously defined function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use  function\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vary the lag and compute the cross-correlation again. Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Rastergram and PSTH\n",
    "\n",
    "\n",
    "## 3.1 Grouping spiking activity per trial\n",
    "\n",
    "It is now time to start **relating spiking activity to the behavior**.\n",
    "First, we need to extract the spiking activity corresponding to each trial. \n",
    "More precisely, on each trial, we want to **identify all the spikes in a certain window around the onset of the animal response and re-reference the spike times w.r.t. this response onset** (i.e. t=0 now corresponds to response onset). We will use a window from 500 ms prior to response onset to 1000 ms after response onset.\n",
    "**Add the spike train per trial as a new variable to our dataframe** (under the name `Spikes`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define window\n",
    "window = ??\n",
    "\n",
    "# number of trials\n",
    "nTrial = ??\n",
    "\n",
    "# initiliaze a list with the spikes corresponding to each trial\n",
    "SpikesPerTrial = []\n",
    "\n",
    "#loop through trials\n",
    "for t in range(nTrial):\n",
    "    \n",
    "    # re-reference time so that time=0 corresponds to tResponse in this trial\n",
    "    SpikeRereferenced = ???\n",
    "    \n",
    "    # select only spikes in window\n",
    "    ????\n",
    "    \n",
    "    # add to SpikesPerTrial\n",
    "    ????\n",
    "\n",
    "# add to the dataframe\n",
    "???\n",
    "\n",
    "#check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Rastergram\n",
    "Now we can see how the activity of the neuron evolves within each trial by plotting a rastergram (or rasterplot).\n",
    "In a rastergram, x-axis denotes time, y-axis denotes the different trials, and each spike is noted by a vertical tick (or point). A rastergram can also be used to plot the activity of a population if neurons across a single trial, and so the vertical axis denotes neuron identity.\n",
    "**Plot the rastergram for this neuron.**\n",
    "Add a vertical line to mark response onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loop through trials\n",
    "for ????\n",
    "    \n",
    "    # number of spikes in corresponding window    \n",
    "    nSpikes = ???\n",
    "    \n",
    "    # plot spike as blue vertical ticks\n",
    "    plt.plot(??, ??,'b|');\n",
    "\n",
    "# add line at response onset\n",
    "plt.axvline(??);\n",
    "\n",
    "# add labels\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of variability of neuron firing across trials. But importantly, in many trials we also see an increase of activity starting 200 ms approx after motion onset (although that increase does not seem to be present in all trials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Rastergram per condition\n",
    "The rastergram can also be separated by behavioral condition, to inspect visually whether the neural activity is related to some behavioral condition. \n",
    "**Plot the PSTH separately for trials with leftward and rightwards responses.**\n",
    "(In practice, we use a different color for each type of response and we plot  trials in one condition above trials in the other condition, see an example [here](https://i.gyazo.com/06659ec5318b06dd3d82ef2fbd15948c.png) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify trials with leftwards response (i.e. =1)\n",
    "TrialIndexResp1 = ???\n",
    "\n",
    "# number of leftward responses\n",
    "nResp1 = ???\n",
    "\n",
    "# loop through leftward response trials\n",
    "for t in ??:\n",
    "    # corresponding index of trial in list of ALL trials\n",
    "    tt = ??\n",
    "    \n",
    "    # plot spikes for this trial (in blue)\n",
    "    nSpikes = ??\n",
    "    plt.plot(???);\n",
    "\n",
    "\n",
    "# repeat the same for trials with rightwards responses (do not forget to change the color and add a vertical offset)\n",
    "?????????????????????????????????    \n",
    "\n",
    "    \n",
    "# add line at response onset\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 PSTH\n",
    "We can confirm our visual impression of rastergrams by plotting the Peri-Stimulus Time Histogram (or PSTH), which is an ugly name to say that we basically average the spike counts across trials, after binning the spikes using a certain window. By averaging activity across trials, we also gain statistical power to detect consistent changes in activity at a given time in a given condition.\n",
    "\n",
    "First, **define a function that computes the PSTH for a given array of spike trains**. For this analysis we don't care about the trial identity of each spike, only their timing, so we can concatenate spike trains across trials using `np.concatenate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will compute the PSTH of a given array of spike trains\n",
    "def compute_PSTH(S, dt, Tini, Tend):\n",
    "    \"\"\"\n",
    "    Computes the PSTH. \n",
    "    Args:\n",
    "       S (list of ndarrays): list of spike trains (one array per trial)\n",
    "       dt (float): time resolution for PSTH\n",
    "       Tini (float): initial time of PSTH\n",
    "       Tend (float): final time of PSTH\n",
    "    Returns:\n",
    "       ndarray: PSTH\n",
    "       ndarray: corresponding time bin centers\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of trials in the array\n",
    "    nTrial = ??\n",
    "    \n",
    "    # concatenate all spikes in one single array\n",
    "    all_spike = ???\n",
    "        \n",
    "    # define the time grid, starting at Tini until Tend with dt resolution\n",
    "    time_grid = ??\n",
    "\n",
    "    # compute the spike counts in all bins defined by the time grid\n",
    "    H = ???\n",
    "    \n",
    "    # the first element is the PSTH, the second is the time grid\n",
    "    PSTH = H[0]\n",
    "    \n",
    "    # normalize the counts by number of trials and size of time bin so that PSTH is defined in spikes/sec\n",
    "    norm_factor = ??\n",
    "    PSTH = PSTH / norm_factor\n",
    "\n",
    "    # remove last element in time grid to match length with PSTH\n",
    "    time_grid = ??\n",
    "    \n",
    "    return PSTH, time_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the PSTH separately for leftward and rightward trials** (as two curves on the same plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time resolution\n",
    "dt = .01\n",
    "\n",
    "# spike trains for leftward and rightward response trials\n",
    "# we need '.values' to convert dataframe back to array\n",
    "Spikes_resp1 = df.??[??].values\n",
    "Spikes_resp2 = df.??[??].values\n",
    "\n",
    "# compute PSTH using the function above\n",
    "???????\n",
    "\n",
    "# plot\n",
    "plt.plot(???????, label = \"left\");\n",
    "plt.plot(??????, label = \"right\");\n",
    "\n",
    "# vertical line\n",
    "???\n",
    "\n",
    "# add labels\n",
    "???\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Smooth PSTH (Optional)\n",
    "Because very fast changes in the spiking activity are often irrelevant, **PSTHs are often smoothed by convolving it with a gaussian kernel**. The width of the gaussian controls the time scale of smoothing. The functions below define a gaussian kernel and smooth a signal using this gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(sigma):\n",
    "    \"\"\"\n",
    "    Defines a gaussian kernel. \n",
    "    Args:\n",
    "       sigma (float): width of gaussian kernel.\n",
    "    Returns:\n",
    "       ndarray: gaussian kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    #define the length of the kernel\n",
    "    kernel_length = 5*sigma\n",
    "    \n",
    "    # define indices for the kernel\n",
    "    time_vec = np.arange(-3*sigma,3*sigma) \n",
    "\n",
    "    #generate a Gaussian kernel (a value for each index)\n",
    "    K = norm.pdf(time_vec, loc=0, scale=sigma)\n",
    "    \n",
    "    #Normalize the kernel to have area=1 to maintain the same rate units\n",
    "    K = K/np.sum(K)\n",
    "    \n",
    "    return K\n",
    "\n",
    "    \n",
    "def smooth_function(X, dt, sigma):\n",
    "    \"\"\"\n",
    "    Smooths any time series with a gaussian kernel. \n",
    "    Args:\n",
    "       X (ndarray): time series.\n",
    "       dt (float): time resolution of time series\n",
    "       sigma (float): width of gaussian kernel.\n",
    "    Returns:\n",
    "       ndarray: Smoothed time series\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert sigma of kernel to time bins\n",
    "    sigma_timebins = int(sigma/dt)\n",
    "\n",
    "    # define gaussian kernel\n",
    "    K = gaussian_kernel(sigma_timebins)\n",
    "    \n",
    "    # convolve signal with kernel\n",
    "    Xsmooth = np.convolve(X, K, 'same')\n",
    "\n",
    "    return Xsmooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply plots the gaussian kernel, so you get an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10 # here define in time bins\n",
    "\n",
    "plt.plot(gaussian_kernel(sigma));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **apply the function to smooth the PSTHs**. Choose an appropriate value for sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #define the temporal width (in SECONDS) of the Gaussian kernel to filter the PSTH\n",
    "sigma_kernel = ??\n",
    "\n",
    "# use the function above to convolve the two PSTHS\n",
    "SmoothPSTH_Resp1 = ??\n",
    "SmoothPSTH_Resp2 = ??\n",
    "\n",
    "# plot the smoothed PSTHs\n",
    "?????????\n",
    "\n",
    "# vertical line\n",
    "????\n",
    "\n",
    "# labels\n",
    "??????\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fano Factor\n",
    "\n",
    "The Fano Factor is a **measure of the reliability of the spiking process**. In other words it tells us how stochastic spiking is. It is computed on spike counts on a window long enough to have on average more than one spike. The value is $FF = \\frac{V}{M}$, where $V$ is the variance of the spike count across trials (windows), and $M$ is the mean of the spike count across trials.The homogeneous Poisson process has a Fano Factor equal to 1. Larger values means even more random than that (i.e. highly bursting), lower values means more regular firing (i.e. periodic).\n",
    "The Fano Factor must be computed after taking into account the factors that we know affect the activity of the neuron (and so introduce variability in the spiking). Which is why we will compute separately for leftward and rightward responses.\n",
    "\n",
    "**Compute the number of spikes in each trial in a window of 500 ms starting at response onset**, and as a variable to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window\n",
    "MotorWindow = ??\n",
    "\n",
    "# Pre-allocate spike count array\n",
    "SC = np.zeros(??)\n",
    "\n",
    "#loop through trials\n",
    "for ????\n",
    "    # number of spike counts within this window for this trial\n",
    "    SC[t] = ????????\n",
    "\n",
    "# add to dataframe\n",
    "??\n",
    "\n",
    "# check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute separately for leftward and rightward responses the average spike count, variance of the spike count and Fano Factor. Plot all values in bar plots.** (Note: the variance is computed using `np.var`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T17:08:40.528444Z",
     "start_time": "2021-03-09T17:08:40.073919Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pre-allocate numpy arrays for mean, variance of spike counts and Fano Factor\n",
    "MeanCount = np.zeros(2)\n",
    "VarCount = np.zeros(2)\n",
    "FanoFactor  = np.zeros(2)\n",
    "\n",
    "# for each type of response (leftward/rightward)\n",
    "for r in range(2):\n",
    "    \n",
    "    # corresponding trials (mind the values taken by df.Response!)\n",
    "    mask = ???\n",
    "    \n",
    "    # corresponding spike counts\n",
    "    SpikeCountResp = ??\n",
    "    \n",
    "    # mean values\n",
    "    MeanCount[r] = ??\n",
    "    \n",
    "    # variance\n",
    "    VarCount[r] = ??\n",
    "    \n",
    "    # Fano Factor\n",
    "    FanoFactor[r] = ??\n",
    "\n",
    "# reponse label\n",
    "RespLabel = ('Left','Right')\n",
    "\n",
    "# plot mean rate\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(????)\n",
    "plt.ylabel(??);\n",
    "plt.xlabel(??)\n",
    "\n",
    "# plot variance\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(????)\n",
    "plt.ylabel(??);\n",
    "plt.xlabel(??)\n",
    "\n",
    "# plot Fano Factor\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(????)\n",
    "plt.ylabel(??);\n",
    "plt.xlabel(??)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spike count regression\n",
    "\n",
    "In this last part, we will see how we can perform **regression on spike counts data to determine the variables that a neuron is encoding**. This will be done using **Poisson regression** or **Poisson GLM**, which is the standard regression for count data (just as logistic regression is for binary data).\n",
    "\n",
    "## 5.1 Logistic regression\n",
    "\n",
    "But first, let us do look at how the animal behaves during this session. We have copied below (and adapted, thank you) the functions used in Assignments 2-3 to plot and fit a psychometric curve. Read through them if you need a little refresh on logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    \"\"\"\n",
    "    Returns the output of the logistic function for the given input value (float or array-like).\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def psychometric_model(d, w0, w1):\n",
    "    \"\"\"\n",
    "    Compute the psychometric function based on a simple logistic model. \n",
    "    Args:\n",
    "       d (ndarray): input values.\n",
    "       w0 (float): intercept for logistic regression.\n",
    "       w1 (float): slope for logistic regression.\n",
    "    Returns:\n",
    "       ndarray: The `y` data points of the psychometric function. \n",
    "          In our case, this corresponds to the probability of CCW responses.\n",
    "    \"\"\"\n",
    "    p = logistic(w0 + w1*d)\n",
    "    return p\n",
    "\n",
    "def plotcurve(df, color):\n",
    "    \"\"\"\n",
    "    Plot the fitted psychometric curve with experimental datapoints on top. \n",
    "    Args:\n",
    "       df (dataframe): experimental data\n",
    "       color (string): color of the datapoints and fitted line\n",
    "    \"\"\"\n",
    "\n",
    "    mod = smf.glm(formula='Response ~ Stimulus', data=df, family=sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    \n",
    "    myx = np.linspace(-2,2,100)\n",
    "    yfit=res.predict(pd.DataFrame({'Stimulus': myx})) #yfit = res.predict(exog={'probe_target':myx})\n",
    "\n",
    "    # plot the psychometric function (fit)\n",
    "    plt.plot(myx,yfit,'-', color=color,label='fit')\n",
    "    \n",
    "    # plot the psychometric curve (datapoints)\n",
    "    df.groupby('Stimulus').Response.agg(('mean','sem')).plot(yerr='sem', color=color, fmt = 'o', ax=plt.gca(), label='data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying logistic regression, we need to make sure that our dependent variable (here the response) is coded as 0s and 1s. Now this is not the case. **Change the values so that 1s mark rightward responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract 1, so 2 becomes 1 (right) and 1 becomes 0 (left)\n",
    "?????\n",
    "\n",
    "#check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **use the functions above to plot the psychometric curve.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotcurve(???)\n",
    "\n",
    "plt.xlabel('stimulus evidence');\n",
    "plt.ylabel('p(rightward response)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Simple Poisson regression\n",
    "Now we are ready for a simple regression of the neuron spike count (in the 500-ms response window).\n",
    "**Regress the spike counts against stimulus evidence using a Poisson GLM**. In practice, this is done just like logistic regression using `glm` from `statsmodel` package, but setting the family to `Poisson` instead of `Binomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod = smf.glm(formula=???, data=??, family=sm.families.Poisson())\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Multiple Poisson regression\n",
    "The previous model only looked at the possible influene of stimulus onto spiking, but as for other GLMs we can add more factors in by changing the formula.\n",
    "**Add a regressor for the response in the GLM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod = ???\n",
    "res = ????\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret the weights for the response. What about the stimulus weight?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 (Optional) Validation a Poisson regression model\n",
    "Finally, as for any statistical model, it is important after fitting the model to validate it by checking if the behavior it predicts deviates or not from what is observed. This is equivalent to validating a simple logistic regression model by comparing the experimental and and fitted psychometric curves.\n",
    "\n",
    "In Poisson regression, we can check whether the spike counts follow an exponential distribution, when experimental variables are controlled. Here, since stimulus plays no role in the spiking activity, we can **plot the distribution of spike counts separately for leftward and rightward responses against the predictions from the Poisson model**. The Poisson model predicts exponential distributions with a scale defined by the rate of the model.\n",
    "\n",
    "Reminder: in the Poisson GLM, the rate (or expected value) is the exponential of the weighted sum of the regressors (use `np.exp`). The exponential is the non-linearity of the Poisson GLM, as the logistic function is non-linearity of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExpectedCount_Response = np.zeros(2)\n",
    "\n",
    "# expected value of counts for leftward responses\n",
    "ExpectedCount_Response[0] = ??\n",
    "\n",
    "# expected value of counts for rightward responses\n",
    "ExpectedCount_Response[1] = ??\n",
    "\n",
    "# loop through left/right responses\n",
    "for r in ??:\n",
    "    # corresponding subplot\n",
    "    plt.subplot(1,2,r+1)\n",
    "    \n",
    "    # select corresponding trials\n",
    "    mask = ??\n",
    "    \n",
    "    #corresponding spike counts\n",
    "    SpikeCountResp = ??\n",
    "    \n",
    "    # plot experimental distribution of counts\n",
    "    ???\n",
    "    \n",
    "    # range of values to plot for exponential distribution\n",
    "    xx = range(20)\n",
    "    \n",
    "    # scale is inverse of expected count\n",
    "    scale = ??\n",
    "    \n",
    "    #probability of count according to Poisson distribution \n",
    "    pPoisson = expon.pdf(???)\n",
    "    \n",
    "    # plot Poisson prediction\n",
    "    ???\n",
    "    \n",
    "    plt.xlabel('spike count');\n",
    "    if r==0: plt.ylabel('density')  \n",
    "    plt.legend();\n",
    "    plt.title(RespLabel[r]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
